{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPn32mSRB55PyINttGlgGxz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctrivino1/YLearn/blob/main/Feature_Importance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4 ways to find feature importance**\n",
        "\n",
        "## 1.   **Correlation Matrix:**\n",
        "- Provides information about linear relationships between features. Useful for identifying multicollinearity.\n",
        "\n",
        "## 2.   **XGBoost feature importance:**\n",
        "- Is based on the internal structure and performance of the model, reflecting how often and how much each feature is used in the ensemble of trees.\n",
        "\n",
        "## 3.    **Permutation importance:**\n",
        "- Measures the decrease in model performance when the values of a specific feature are randomly shuffled.\n",
        "\n",
        "##4.   **Shap feature importance:**\n",
        "- SHAP values provide insights not only into feature importance but also into the direction and impact of each feature on individual predictions.\n",
        "- Shap values can also be calculated with permutation, but it was much more computationally expensive compared to sklearn's permutation_importance functionality\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mJuSf3MwmNcp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us6T2wDkxgWV"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "# correlation matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load California housing dataset\n",
        "california_housing = fetch_california_housing()\n",
        "X = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n",
        "y = california_housing.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fit XGBoost Regressor\n",
        "xgb = XGBRegressor(n_estimators=100)\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = xgb.feature_importances_\n",
        "y_pred = xgb.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f'MAE: {mae}')\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f'RMSE: {rmse}')\n",
        "\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(f'R-squared: {r_squared:.4f}')\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def correlation_heatmap(data):\n",
        "    correlations = data.corr()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f', cmap=\"YlGnBu\",\n",
        "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70}\n",
        "                )\n",
        "    plt.show()\n",
        "\n",
        "# Assuming X_train is your training data\n",
        "correlation_heatmap(X_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# xgb feature importance WITHOUT NORMALIZATION\n",
        "# Visualize importances\n",
        "sorted_idx = feature_importances.argsort()\n",
        "plt.barh(X.columns[sorted_idx], feature_importances[sorted_idx])\n",
        "plt.xlabel(\"XGBoost Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aGCv_fQIjsQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xgbregresssor WITH NORMALIZATION\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform on training data (the splits were done in the first code block)\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform test data (using the same scaling factors)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Fit XGBoost Regressor on scaled data\n",
        "xgb = XGBRegressor(n_estimators=100)\n",
        "xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = xgb.feature_importances_\n",
        "y_pred = xgb.predict(X_test_scaled)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f'MAE: {mae}')\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f'RMSE: {rmse}')\n",
        "\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(f'R-squared: {r_squared:.4f}')\n",
        "\n",
        "\n",
        "# Visualize importances\n",
        "sorted_idx = feature_importances.argsort()\n",
        "plt.barh(X.columns[sorted_idx], feature_importances[sorted_idx])\n",
        "plt.xlabel(\"XGBoost Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yCR0A-is_vR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Permutation feature importance\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Perform permutation feature importance\n",
        "perm_importance = permutation_importance(xgb, X_test, y_test, n_repeats=300, random_state=42)\n",
        "\n",
        "# Visualize importances\n",
        "sorted_idx = perm_importance.importances_mean.argsort()\n",
        "plt.barh(X.columns[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
        "plt.xlabel(\"Permutation Importance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pDEQtxCXx0xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "svEv4m-Xyr_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shap values\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "import shap\n",
        "\n",
        "# Create a SHAP explainer\n",
        "explainer = shap.TreeExplainer(xgb)\n",
        "\n",
        "# Calculate SHAP values\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Summary plot of SHAP feature importances\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n"
      ],
      "metadata": {
        "id": "f3wYfopWyfDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shap permutation\n",
        "import shap\n",
        "from sklearn.ensemble import RandomForestRegressor  # Replace with your model import\n",
        "\n",
        "# Assume you have a trained XGBoost Regressor\n",
        "xgb = XGBRegressor(n_estimators=100)\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Masker function for background data (replace with your own if needed)\n",
        "background_data = shap.maskers.Independent(X_train, max_samples=100)\n",
        "\n",
        "# Create a PermutationExplainer\n",
        "explainer = shap.PermutationExplainer(xgb.predict, background_data)  # Pass the predict method\n",
        "\n",
        "# Explain Shapley values for the test set\n",
        "shap_values = explainer.shap_values(X_test, npermutations=3)\n",
        "\n",
        "# Explain a single row\n",
        "#row_index = 0  # Replace with the index of the row you want to explain\n",
        "#shap_values = explainer.shap_values(X.iloc[row_index, :])\n",
        "\n",
        "\n",
        "# Visualize the explanations\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "shap.summary_plot(shap_values, X_test)  # Adjust parameters as needed\n"
      ],
      "metadata": {
        "id": "bnNBzB4hqneg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HNzQic2xsyxG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}